<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Shuo Li UPenn</title>
    <link rel="stylesheet" href="css/style.css">
</head>
<body>
    <header>
        <div class="container">
            <img src="images/personal.png" alt="Shuo Li" class="profile-img">
            <h1>Shuo Li</h1>
            <p> Computer Science | University of Pennsylvania</p>
            <p>Email: <a href="mailto:lishuo1@seas.upenn.edu">lishuo1@seas.upenn.edu</a></p>
            <p>
                <a href="https://scholar.google.com/citations?user=-QaDf40AAAAJ&hl=en" target="_blank">
                    <img src="images/google-scholar.png" alt="Google Scholar" class="icon"> Google Scholar
                </a>
                |
                <a href="https://www.linkedin.com/in/shuo-li-penn" target="_blank">
                    <img src="images/linkedin.png" alt="LinkedIn" class="icon"> LinkedIn
                </a>
            </p>
        </div>
    </header>

    <nav>
        <div class="container">
            <ul>
                <li><a href="#about">About Me</a></li>
                <li><a href="#research">Research</a></li>
                <li><a href="#publications">Publications</a></li>
                <!-- <li><a href="#teaching">Teaching</a></li> -->
                <li><a href="#contact">Contact</a></li>
            </ul>
        </div>
    </nav>

    <section id="about" class="section">
        <div class="container">
            <h2>About Me</h2>
            <p>I am a final-year Ph.D. student at the University of Pennsylvania, under the guidance of Insup Lee and Osbert Bastani. My research is dedicated to enhancing the safety of machine learning, deep learning, and large language models (LLMs). Currently, my work focuses on LLMs for code, safety alignment, and uncertainty quantification. In previous projects, I developed safe AI solutions for anomaly and object detection. My contributions have been recognized at top conferences, including NeurIPS, ICLR, ICML, NAACL, EMNLP, and KDD. My long-term ambition is to advance the development of safe and secure ML, DL, and LLM systems. I am actively seeking job opportunities and collaborations, and I would be delighted to connect if you're interested in discussing potential opportunities.</p>
        </div>
    </section>

    <section id="research" class="section">
        <div class="container">
            <h2>Research Interests</h2>
            <ul>
                <li>AI Safety</li>
                <li>NLP</li>
                <li>LLM4Code</li>
                <li>Uncertainty Quantification</li>
            </ul>
        </div>
    </section>

    <section id="publications" class="section">
        <div class="container">
            <h2>Representative Publications</h2>
    
            <div class="publication">
                <img src="images/traq.png" alt="TRAQ" class="paper-img">
                <div class="paper-details">
                    <h3>TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction
                    </h3>
                    <p><strong>TL;DR:</strong> TRAQ utilizes Conformal prediction and Bayesian optimization to guarantee the correcntess of RAG.</p>
                    <p><strong>Abstract:</strong> When applied to open-domain question answering, large language models (LLMs) frequently generate incorrect responses based on made-up facts, which are called \textit{hallucinations}. Retrieval augmented generation (RAG) is a promising strategy to avoid hallucinations, but it does not provide guarantees on its correctness. To address this challenge, we propose the Trustworthy Retrieval Augmented Question Answering, or \textit{TRAQ}, which provides the first end-to-end statistical correctness guarantee for RAG. TRAQ uses conformal prediction, a statistical technique for constructing prediction sets that are guaranteed to contain the semantically correct response with high probability. Additionally, TRAQ leverages Bayesian optimization to minimize the size of the constructed sets. In an extensive experimental evaluation, we demonstrate that TRAQ provides the desired correctness guarantee while reducing prediction set size by 16.2\% on average compared to an ablation.</p>
                    <p><strong>Venue:</strong> Accepted at NAACL, 2024</p>
                    <p><a href="https://arxiv.org/abs/2307.04642" target="_blank">Read Full Paper</a></p>
                </div>
            </div>
    
            <div class="publication">
                <img src="images/MoCAN.png" alt="MoCAN" class="paper-img">
                <div class="paper-details">
                    <h3>One-Shot Safety Alignment for Large Language
                        Models via Optimal Dualization</h3>
                    <p><strong>TL;DR:</strong> We propose an one-shot safety alignment algorithm for LLM safety and helpfulness alignment.</p>
                    <p><strong>Abstract:</strong> The growing safety concerns surrounding Large Language Models (LLMs) raise an urgent need to align them with diverse human preferences to simultaneously enhance their helpfulness and safety. A promising approach is to enforce safety constraints through Reinforcement Learning from Human Feedback (RLHF). For such constrained RLHF, common Lagrangian-based primal-dual policy optimization methods are computationally expensive and often unstable. This paper presents a dualization perspective that reduces constrained alignment to an equivalent unconstrained alignment problem. We do so by pre-optimizing a smooth and convex dual function that has a closed form. This shortcut eliminates the need for cumbersome primal-dual policy iterations, thus greatly reducing the computational burden and improving training stability. Our strategy leads to two practical algorithms in model-based and preference-based scenarios (MoCAN and PeCAN, respectively). A broad range of experiments demonstrate the effectiveness of our methods.</p>
                    <p><strong>Venue:</strong> Submitted to Neurips, 2024</p>
                    <p><a href="https://arxiv.org/abs/2405.19544" target="_blank">Read Full Paper</a></p>
                </div>
            </div>
            
            <div class="publication">
                <img src="images/rank.png" alt="Rank" class="paper-img">
                <div class="paper-details">
                    <h3>Uncertainty in Language Models: Assessment through Rank-Calibration</h3>
                    <p><strong>TL;DR:</strong> We propose a novel LLM uncertainty quantification metric from the perspective of monotonicity.</p>
                    <p><strong>Abstract:</strong> Language Models (LMs) have shown promising performance in natural language generation. However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition to verbalized confidence elicited via prompting, many uncertainty measures (e.g., semantic entropy and affinity-graph-based measures) have been proposed. However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges (e.g., [0,âˆž) or [0,1]). In this work, we address this issue by developing a novel and practical framework, termed Rank-Calibration, to assess uncertainty and confidence measures for LMs. Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average. Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score (e.g., ROUGE or METEOR). The broad applicability and the granular interpretability of our methods are demonstrated empirically.</p>
                    <p><strong>Venue:</strong> Submitted to EMNLP, 2024</p>
                    <p><a href="https://arxiv.org/abs/2404.03163" target="_blank">Read Full Paper</a></p>
                </div>
            </div>

            <div class="publication">
                <img src="images/great_figure.png" alt="Rank" class="paper-img">
                <div class="paper-details">
                    <h3>PAC-Wrap: Semi-Supervised PAC Anomaly Detection</h3>
                    <p><strong>TL;DR:</strong> We guarantee the false positive and false negative rates of anomaly detection algorithms via conformal prediction.</p>
                    <p><strong>Abstract:</strong> Anomaly detection is essential for preventing hazardous outcomes for safety-critical applications like autonomous driving. Given their safety-criticality, these applications benefit from provable bounds on various errors in anomaly detection. To achieve this goal in the semi-supervised setting, we propose to provide Probably Approximately Correct (PAC) guarantees on the false negative and false positive detection rates for anomaly detection algorithms. Our method (PAC-Wrap) can wrap around virtually any existing semi-supervised and unsupervised anomaly detection method, endowing it with rigorous guarantees. Our experiments with various anomaly detectors and datasets indicate that PAC-Wrap is broadly effective.</p>
                    <p><strong>Venue:</strong> Accepted to KDD, 2022</p>
                    <p><a href="https://dl.acm.org/doi/pdf/10.1145/3534678.3539408" target="_blank">Read Full Paper</a></p>
                </div>
            </div>
    
            <!-- Add more papers in the same format -->
    
        </div>
    </section>

    <!-- <section id="teaching" class="section">
        <div class="container">
            <h2>Teaching</h2>
            <p>Include information about courses you have taught or are currently teaching. You can also link to course materials.</p>
        </div>
    </section> -->

    <section id="contact" class="section">
        <div class="container">
            <h2>Contact</h2>
            <p>Office: 3330 Walnut Street, Philadelphia, PA</p>
            <p>Telephone: 445-888-2015</p>
            <p>Email: <a href="mailto:lishuo1@seas.upenn.edu">lishuo1@seas.upenn.edu</a></p>
        </div>
    </section>
    
    <footer>
        <div class="container">
            <p>&copy; 2024 Shuo Li. All rights reserved.</p>
        </div>
    </footer>
</body>
</html>
