<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Your Name - Academic Personal Website</title>
    <link rel="stylesheet" href="css/style.css">
</head>
<body>
    <header>
        <div class="container">
            <img src="images/profile.jpg" alt="Your Name" class="profile-img">
            <h1>Shuo Li</h1>
            <p> Computer Science | University of Pennsylvania</p>
            <p>Email: <a href="mailto:lishuo1@seas.upenn.edu">lishuo1@seas.upenn.edu</a></p>
            <p>
                <a href="https://scholar.google.com/citations?user=-QaDf40AAAAJ&hl=en" target="_blank">
                    <img src="images/google-scholar.png" alt="Google Scholar" class="icon"> Google Scholar
                </a>
                |
                <a href="https://www.linkedin.com/in/shuo-li-penn" target="_blank">
                    <img src="images/linkedin.png" alt="LinkedIn" class="icon"> LinkedIn
                </a>
            </p>
        </div>
    </header>

    <nav>
        <div class="container">
            <ul>
                <li><a href="#about">About Me</a></li>
                <li><a href="#research">Research</a></li>
                <li><a href="#publications">Publications</a></li>
                <!-- <li><a href="#teaching">Teaching</a></li> -->
                <li><a href="#contact">Contact</a></li>
            </ul>
        </div>
    </nav>

    <section id="about" class="section">
        <div class="container">
            <h2>About Me</h2>
            <p>I am a final-year Ph.D. student at the University of Pennsylvania, advised by Insup Lee and Osbert Bastani. My research focuses on enhancing the safety of machine learning, deep learning, and large language models (LLMs). Recently, I have worked on LLMs for code, safety alignment, and uncertainty quantification. In previous projects, I contributed to developing safe AI for anomaly and object detection. My work has been accepted at top conferences, including NeurIPS, ICLR, ICML, NAACL, EMNLP, and KDD. My long-term goal is to develop safe and secure ML, DL, and LLM systems.</p>
        </div>
    </section>

    <section id="research" class="section">
        <div class="container">
            <h2>Research Interests</h2>
            <ul>
                <li>Research Area 1</li>
                <li>Research Area 2</li>
                <li>Research Area 3</li>
            </ul>
        </div>
    </section>

    <section id="publications" class="section">
        <div class="container">
            <h2>Publications</h2>
    
            <div class="publication">
                <img src="images/traq.png" alt="TRAQ" class="paper-img">
                <div class="paper-details">
                    <h3>TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction
                    </h3>
                    <p><strong>TL;DR:</strong> TRAQ utilizes Conformal prediction and Bayesian optimization to guarantee the correcntess of RAG.</p>
                    <p><strong>Abstract:</strong> When applied to open-domain question answering, large language models (LLMs) frequently generate incorrect responses based on made-up facts, which are called \textit{hallucinations}. Retrieval augmented generation (RAG) is a promising strategy to avoid hallucinations, but it does not provide guarantees on its correctness. To address this challenge, we propose the Trustworthy Retrieval Augmented Question Answering, or \textit{TRAQ}, which provides the first end-to-end statistical correctness guarantee for RAG. TRAQ uses conformal prediction, a statistical technique for constructing prediction sets that are guaranteed to contain the semantically correct response with high probability. Additionally, TRAQ leverages Bayesian optimization to minimize the size of the constructed sets. In an extensive experimental evaluation, we demonstrate that TRAQ provides the desired correctness guarantee while reducing prediction set size by 16.2\% on average compared to an ablation.</p>
                    <p><strong>Venue:</strong> Accepted at NAACL, 2024</p>
                    <p><a href="https://arxiv.org/abs/2307.04642" target="_blank">Read Full Paper</a></p>
                </div>
            </div>
    
            <div class="publication">
                <img src="images/MoCAN.png" alt="MoCAN" class="paper-img">
                <div class="paper-details">
                    <h3>One-Shot Safety Alignment for Large Language
                        Models via Optimal Dualization</h3>
                    <p><strong>TL;DR:</strong> Brief summary of the paper in a few words.</p>
                    <p><strong>Abstract:</strong> The growing safety concerns surrounding Large Language Models (LLMs) raise an urgent need to align them with diverse human preferences to simultaneously enhance their helpfulness and safety. A promising approach is to enforce safety constraints through Reinforcement Learning from Human Feedback (RLHF). For such constrained RLHF, common Lagrangian-based primal-dual policy optimization methods are computationally expensive and often unstable. This paper presents a dualization perspective that reduces constrained alignment to an equivalent unconstrained alignment problem. We do so by pre-optimizing a smooth and convex dual function that has a closed form. This shortcut eliminates the need for cumbersome primal-dual policy iterations, thus greatly reducing the computational burden and improving training stability. Our strategy leads to two practical algorithms in model-based and preference-based scenarios (MoCAN and PeCAN, respectively). A broad range of experiments demonstrate the effectiveness of our methods.</p>
                    <p><strong>Venue:</strong> Submitted to Neurips, 2024</p>
                    <p><a href="https://arxiv.org/abs/2405.19544" target="_blank">Read Full Paper</a></p>
                </div>
            </div>
            
            <div class="publication">
                <img src="images/rank.png" alt="Rank" class="paper-img">
                <div class="paper-details">
                    <h3>Uncertainty in Language Models: Assessment through Rank-Calibration</h3>
                    <p><strong>TL;DR:</strong> Brief summary of the paper in a few words.</p>
                    <p><strong>Abstract:</strong> Language Models (LMs) have shown promising performance in natural language generation. However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition to verbalized confidence elicited via prompting, many uncertainty measures (e.g., semantic entropy and affinity-graph-based measures) have been proposed. However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges (e.g., [0,âˆž) or [0,1]). In this work, we address this issue by developing a novel and practical framework, termed Rank-Calibration, to assess uncertainty and confidence measures for LMs. Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average. Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score (e.g., ROUGE or METEOR). The broad applicability and the granular interpretability of our methods are demonstrated empirically.</p>
                    <p><strong>Venue:</strong> Submitted to EMNLP, 2024</p>
                    <p><a href="https://arxiv.org/abs/2404.03163" target="_blank">Read Full Paper</a></p>
                </div>
            </div>
    
            <!-- Add more papers in the same format -->
    
        </div>
    </section>

    <!-- <section id="teaching" class="section">
        <div class="container">
            <h2>Teaching</h2>
            <p>Include information about courses you have taught or are currently teaching. You can also link to course materials.</p>
        </div>
    </section> -->

    <section id="contact" class="section">
        <div class="container">
            <h2>Contact</h2>
            <p>Office: 3330 Walnut Street, Philadelphia, PA</p>
            <p>Telephone: 445-888-2025</p>
            <p>Email: <a href="mailto:lishuo1@seas.upenn.edu">lishuo1@seas.upenn.edu</a></p>
        </div>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2024 Your Name. All rights reserved.</p>
        </div>
    </footer>
</body>
</html>
